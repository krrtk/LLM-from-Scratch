{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b6c481b-ac54-4531-a54d-1ed765760a9e",
   "metadata": {},
   "source": [
    "# **Step 1: Creating Token**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350d75db-4d10-4331-a933-3c625bee538e",
   "metadata": {},
   "source": [
    "Loading the dataset that we need to work on (it is a short story dataset by edhit wharton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e6730bf-bf88-4737-a260-672b0744d2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "    raw_text=f.read()\n",
    "print(\"Total number of character:\",len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74a9d83c-3ee3-4eda-9bdd-f4e185b3680e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', ' ', 'world.', ' ', 'this', ' ', 'is', ' ', 'a', ' ', 'test']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text=\"hello world. this is a test\"\n",
    "result= re.split(r'(\\s)',text) #rsplits from all the white spaces\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22bc781b-8840-44df-9cc5-f80632a3f8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', ' ', 'world', '.', '', ' ', 'this', ' ', 'is', ' ', 'a', ' ', 'test']\n"
     ]
    }
   ],
   "source": [
    "result=re.split(r'([,.]|\\s)',text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5ae56ab-50a8-4cdd-92ee-9ab45cca4a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world', '.', 'this', 'is', 'a', 'test']\n"
     ]
    }
   ],
   "source": [
    "result=[item for item in result if item.strip()] #remove white space in the list\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683702b6-cb71-4de8-941e-6e7fb0ec3284",
   "metadata": {},
   "source": [
    "in tokenization to keep or to not keep spaces it all depends on the problem statemenet that we are dealing with that means some problems that need exactly same structure as the input require white spaces, for example a python code is space sensitive therefore in that case we need not to remove white spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e7789a-2279-4ccf-a1cc-a9279766e9d9",
   "metadata": {},
   "source": [
    "**Appplying this tokenizing knowledge on our dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6450f09-645a-4dde-9a58-309eeef01a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "token= re.split(r'([,.;!_?@$%^&*()+=\\'\\#]|--|\\s)', raw_text)\n",
    "token=[item for item in token if item.strip()]\n",
    "print(token[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7a64352-8b81-436d-acb4-4efbec825aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4585\n"
     ]
    }
   ],
   "source": [
    "print(len(token))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331ec465-70fe-49de-b35b-ae9e9fefa406",
   "metadata": {},
   "source": [
    "# **Step 2: Creating token IDs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd91668-5fd2-4bf2-852e-74eca4d59fe1",
   "metadata": {},
   "source": [
    "tokenized text -> vocabulary (map all in alphabetical order then give unique ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aae5df78-4d82-499f-9b81-3a2a41b09eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1173\n"
     ]
    }
   ],
   "source": [
    "words= sorted(set(token))\n",
    "vocab_size= len(words)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb2e441-88d2-4c75-8a53-dea664bebe40",
   "metadata": {},
   "source": [
    "thus now we have 1191 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d8e31687-1328-4b33-879e-9147792f39aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab={token:integer for integer, token in enumerate(words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "28b25fdb-0507-4556-b819-2ed9ec319ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "('\"Ah', 2)\n",
      "('\"Be', 3)\n",
      "('\"Begin', 4)\n",
      "('\"By', 5)\n",
      "('\"Come', 6)\n",
      "('\"Destroyed', 7)\n",
      "('\"Don', 8)\n",
      "('\"Gisburns\"', 9)\n",
      "('\"Grindles', 10)\n",
      "('\"Hang', 11)\n",
      "('\"Has', 12)\n",
      "('\"How', 13)\n",
      "('\"I', 14)\n",
      "('\"If', 15)\n",
      "('\"It', 16)\n",
      "('\"Jack', 17)\n",
      "('\"Money', 18)\n",
      "('\"Moon-dancers\"', 19)\n",
      "('\"Mr', 20)\n",
      "('\"Mrs', 21)\n",
      "('\"My', 22)\n",
      "('\"Never', 23)\n",
      "('\"Of', 24)\n",
      "('\"Oh', 25)\n",
      "('\"Once', 26)\n",
      "('\"Only', 27)\n",
      "('\"Or', 28)\n",
      "('\"That', 29)\n",
      "('\"The', 30)\n",
      "('\"Then', 31)\n",
      "('\"There', 32)\n",
      "('\"There:', 33)\n",
      "('\"This', 34)\n",
      "('\"We', 35)\n",
      "('\"Well', 36)\n",
      "('\"What', 37)\n",
      "('\"When', 38)\n",
      "('\"Why', 39)\n",
      "('\"Yes', 40)\n",
      "('\"You', 41)\n",
      "('\"but', 42)\n",
      "('\"deadening', 43)\n",
      "('\"dragged', 44)\n",
      "('\"effects\"', 45)\n",
      "('\"interesting\":', 46)\n",
      "('\"lift', 47)\n",
      "('\"obituary\"', 48)\n",
      "('\"strongest', 49)\n",
      "('\"strongly\"', 50)\n"
     ]
    }
   ],
   "source": [
    "for i,item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i>=50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77f2d67-b68c-4ff8-b04c-8a6abc42485c",
   "metadata": {},
   "source": [
    "**creating tokenizer class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd8b222b-7827-4021-8b6e-74c414ad98ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class tokenize:\n",
    "    def __init__(self,vocab):\n",
    "        self.str_to_int=vocab\n",
    "        self.int_to_str={i:s for s,i in vocab.items()}\n",
    "    def encode(self,text):\n",
    "        token= re.split(r'([,.;!_@$%^&*()+=\\'\\#]|--|\\s)',text)\n",
    "        token=[item.strip() for item in token if item.strip()]\n",
    "        ids=[self.str_to_int[s] for s in token]\n",
    "        return ids\n",
    "    def decode(self,ids):\n",
    "        #recreating the text from tokens\n",
    "        text=\" \".join([self.int_to_str[i] for i in ids])\n",
    "        text=re.sub(r'\\s+([,.?!\"()\\'])',r'\\1',text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc9df9f3-c80d-41a6-8ac2-57e84647173d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[93, 87, 182, 1043, 97, 82, 858, 147, 289, 518, 56, 1042, 147, 533, 57]\n",
      "I HAD always thought Jack Gisburn rather a cheap genius -- though a good.\n"
     ]
    }
   ],
   "source": [
    "tokens=tokenize(vocab)\n",
    "text= \"I HAD always thought Jack Gisburn rather a cheap genius--though a good.\"\n",
    "ids=tokens.encode(text)\n",
    "txt=tokens.decode(ids)\n",
    "print(ids)\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8074c3e9-13c1-44f2-b5b7-977cb25d605f",
   "metadata": {},
   "source": [
    "**what if encounter a awork out of our vocaabulary?**\n",
    "we use <|unk|> & <|endoftext|> tokens in the vocabulary to tckle this situation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fd0d6403-3a61-4085-9c74-9cabe9500d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "alltoken=sorted(list(token))\n",
    "alltoken.extend([\"<|endoftext|>\",\"<|unk|>\"])\n",
    "vocab2={token:integer for integer,token in enumerate(alltoken)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "315d8415-d647-45cb-89fa-0259e0ea9a3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1175"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab2.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e3de3a73-643b-4854-920b-5afc1d5debc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class tokenize:\n",
    "    def __init__(self,cab):\n",
    "        self.str_to_int=cab\n",
    "        self.int_to_str={i:s for s,i in cab.items()}\n",
    "    def encode(self,text):\n",
    "        token= re.split(r'([,.;!_@$%^&*()+=\\'\\#]|--|\\s)',text)\n",
    "        token=[item.strip() for item in token if item.strip()]\n",
    "        token=[item if item in self.str_to_int\n",
    "            else \"<|unk|>\" for item in token]\n",
    "        ids=[self.str_to_int[s] for s in token]\n",
    "        return ids\n",
    "    def decode(self,ids):\n",
    "        #recreating the text from tokens\n",
    "        text=\" \".join([self.int_to_str[i] for i in ids])\n",
    "        text=re.sub(r'\\s+([,.?!\"()\\'])',r'\\1',text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d764de37-f43d-4766-ab19-11bfb2536668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea?<|endoftext|>In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "tokenize=tokenize(vocab2)\n",
    "t1=\"Hello, do you like tea?\"\n",
    "t2=\"In the sunlit terraces of the palace.\"\n",
    "text=\"<|endoftext|>\".join((t1,t2))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "01d8f255-bed9-4615-9dac-c3287105d4ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, do you like <|unk|> the sunlit terraces of the <|unk|>.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize.decode(tokenize.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dd73ed-e13b-439c-b567-2ce9e212f975",
   "metadata": {},
   "source": [
    "# \"BYTE_PAIR_Encoding\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4855fbac-98e1-4697-8a4b-1c855d2d041d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in c:\\users\\hp\\miniconda3\\envs\\tf210\\lib\\site-packages (0.12.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\hp\\miniconda3\\envs\\tf210\\lib\\site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\hp\\miniconda3\\envs\\tf210\\lib\\site-packages (from tiktoken) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\hp\\miniconda3\\envs\\tf210\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\miniconda3\\envs\\tf210\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\miniconda3\\envs\\tf210\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\miniconda3\\envs\\tf210\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2025.6.15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eff818ec-9b85-4800-a70c-67a686f8c38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefc2487-d550-4134-ae90-2af3cc2ffe90",
   "metadata": {},
   "source": [
    "Creating an instance of tokenizer class from tiktoken, it works just like our version of tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6df4ab74-3b14-4a4c-a069-a9bd4ae82dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5b1a2f17-2f3a-40ee-8d21-dfda127f5cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 50256, 818, 262, 4252, 18250, 8812, 2114, 286, 262, 20562, 13]\n"
     ]
    }
   ],
   "source": [
    "text=(\"Hello, do you like tea?<|endoftext|>In the sunlit terraces of the palace.\")\n",
    "integer=tokenizer.encode(text,allowed_special={'<|endoftext|>'})\n",
    "print(integer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4287646d-e100-4aab-b4e4-23ce65f4576b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea?<|endoftext|>In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "string=tokenizer.decode(integer)\n",
    "print(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f2920a-ef44-41c1-9e44-d07e69ebd5bb",
   "metadata": {},
   "source": [
    "_we can clearly see here, it handles both <|unk|> and <|endoftext|> tokens easily, as it is using BPE at root level thus shows no error and runs smoothly, moreover it is also trained on larger dataset_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c9f843bb-b3cb-4472-967a-84023d3c0ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "enc_text=tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ad5221-6753-41c7-8dc8-eef325791fef",
   "metadata": {},
   "source": [
    "# **INPUT-TARGET PAIRS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "60f5b636-dc71-4711-8f9b-79dbc84d0994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [40, 367, 2885, 1464]\n",
      "y:      [367, 2885, 1464, 1807]\n"
     ]
    }
   ],
   "source": [
    "context_size = 4 #length of the input\n",
    "enc_sample=enc_text[:50]\n",
    "#The context_size of 4 means that the model is trained to look at a sequence of 4 words (or tokens) \n",
    "#to predict the next word in the sequence. \n",
    "#The input x is the first 4 tokens [1, 2, 3, 4], and the target y is the next 4 tokens [2, 3, 4, 5]\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ef567467-06f7-4007-b3b9-e8fa6629e128",
   "metadata": {},
   "source": [
    "Processing the inputs along with the targets, which are the inputs shifted by one position, we can then create the next-word prediction tasks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fe0926f2-da1c-400d-ab96-def989ce1c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I ------->  H\n",
      "I H -------> AD\n",
      "I HAD ------->  always\n",
      "I HAD always ------->  thought\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,context_size+1):\n",
    "    context=enc_sample[:i]\n",
    "    desired=enc_sample[i]\n",
    "    print(tokenizer.decode(context),\"------->\",tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f872ea8a-640c-4c77-a7ec-3ccbf751a4ab",
   "metadata": {},
   "source": [
    "the above created is a very simple version for input-target pairing, we will use Pytorch Tensors for actual implementation.\n",
    "Tendors are nothing but 2D matrices which store some data\n",
    "following is the implementation for the same:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c041103-590d-405a-9185-006c4b24aef4",
   "metadata": {},
   "source": [
    "# Dataset & Data-loader "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84d19b5-8c57-4f3e-9954-647afa2ba7b5",
   "metadata": {},
   "source": [
    "we will use 2 2D arrays, one with actual input and one with one place shifted input and all will be row wise related"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbddef81-abf2-4eff-88cc-c713a2375710",
   "metadata": {},
   "source": [
    "Step 1: Tokenize the entire text\n",
    "\n",
    "Step 2: Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "\n",
    "Step 3: Return the total number of rows in the dataset\n",
    "\n",
    "Step 4: Return a single row from the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c13ae4-dd2f-4d69-9fe7-7c51b3744560",
   "metadata": {},
   "source": [
    "Below is the implementation for dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f3c43408-da57-4bf9-8342-418662d2aab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    #txt is sample text, tokenizer is the BPE class , Max_length refers to context size,tride is how long to jump\n",
    "    def __init__(self,txt,tokenizer, max_length,stride):\n",
    "        self.input_ids=[]\n",
    "        self.target_ids=[]\n",
    "\n",
    "        token_ids=tokenizer.encode(txt,allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        for i in range(0, len(token_ids)-max_length,stride):\n",
    "            input_chunk=token_ids[i:i+max_length]\n",
    "            target_chunk=token_ids[i+1:i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return self.input_ids[idx],self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6a42fc-9b67-41fb-8cd1-ff96b89479d6",
   "metadata": {},
   "source": [
    "DataLoader\n",
    "\n",
    "Step 1: Initialize the tokenizer\n",
    "\n",
    "Step 2: Create dataset\n",
    "\n",
    "Step 3: drop_last=True drops the last batch if it is shorter than the specified batch_size to prevent loss spikes during training\n",
    "\n",
    "Step 4: The number of CPU processes to use for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a80944f4-d3de-4d1d-abbe-5f0e450a5e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True,num_workers=0):\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    return dataloader #the input output pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5bbed3-3853-4576-aa88-38f87a6f2122",
   "metadata": {},
   "source": [
    "Testing our dataset and adatloader piplene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0c58e9f8-8c06-4d85-9571-ccdaf622ab2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "afbf46d7-accc-4d1b-986f-c7e0a1230909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
     ]
    }
   ],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "36a81545-9237-4421-a2a4-337539331c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Targets:\n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b117fa-b699-4ba9-a9cd-3e2a6d896d71",
   "metadata": {},
   "source": [
    "# CREATE TOKEN EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "293e08be-9876-4e42-b19d-6921b9688742",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "input_ids = torch.tensor([2, 3, 5, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9d5133af-966a-465b-8e1b-1f32e49445e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabsize = 6\n",
    "layer = 3\n",
    "\n",
    "torch.manual_seed(123)\n",
    "embedding = torch.nn.Embedding(vocabsize, layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a6f67023-c579-45dc-899f-37f93597fea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(embedding.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baed905-546a-417b-8867-526bec8db626",
   "metadata": {},
   "source": [
    "these are the randomly generated embedding weights, which will be further optimised, but thing to notic here is that we have a tensor of 3 features and vocabsize of 6 thus getting weight for each token wrt to the vector embedding features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f3a9f512-d96f-408e-8e77-44b817ccf467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b88b057-9967-400d-a88b-2e376f750c36",
   "metadata": {},
   "source": [
    "# **Postional Embeddings (emebed relative to position)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97554171-8ca1-49bc-9285-a2b76c841c72",
   "metadata": {},
   "source": [
    "Initiate the emebedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9e13ee46-5bc4-4f2d-b876-c8c8d844f354",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabsize=50257\n",
    "output=256\n",
    "\n",
    "embedding_layer=torch.nn.Embedding(vocabsize,output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bf5d92-fff4-42f3-9841-6427f9a0151e",
   "metadata": {},
   "source": [
    "Create dataloader (that is data sampling with sliding window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7c803a1a-1616-4dac-a9ba-dab8db2f2c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len=4\n",
    "dataloader=create_dataloader_v1(raw_text,batch_size=8,max_length=max_len,stride=max_len,shuffle=False)\n",
    "data_iter=iter(dataloader)\n",
    "inputs,targets=next(data_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66683daa-68e5-49ac-adfa-13ef58cabcde",
   "metadata": {},
   "source": [
    "SIMPLE ANALOGY\n",
    "\n",
    "dataloader = tokens of words\n",
    "\n",
    "dataloder-> vector embedding \n",
    "\n",
    "vector embedding = each token with the vector dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca5988d-cba4-4b63-99b1-9000b8244543",
   "metadata": {},
   "source": [
    "_FOR POSITIONAL EMBEDDING_\n",
    "\n",
    "size= context_length*vector_dimensions\n",
    "\n",
    "because it will only embed the position for the given context size, not more not less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b0ffe1d0-3be8-4d12-8a3d-878505b6de6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cadebd59-f98b-41df-8cfd-dedce86afe6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "122f8939-3852-4113-b1b6-6876a3ddddd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = max_len\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8b587c27-1c8a-473e-bac7-a20fcf94c800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "pos_embeddings = pos_embedding_layer(torch.arange(max_len))\n",
    "print(pos_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6e213b6b-0f36-4efe-8ac5-2358dcf645c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "250271d5-1af0-4090-9d46-da99528c888b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f3cc31-411a-4f29-9526-d1326bc8f6f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
