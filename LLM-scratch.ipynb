{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b6c481b-ac54-4531-a54d-1ed765760a9e",
   "metadata": {},
   "source": [
    "# **Step 1: Creating Token**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350d75db-4d10-4331-a933-3c625bee538e",
   "metadata": {},
   "source": [
    "Loading the dataset that we need to work on (it is a short story dataset by edhit wharton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e6730bf-bf88-4737-a260-672b0744d2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "    raw_text=f.read()\n",
    "print(\"Total number of character:\",len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74a9d83c-3ee3-4eda-9bdd-f4e185b3680e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', ' ', 'world.', ' ', 'this', ' ', 'is', ' ', 'a', ' ', 'test']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text=\"hello world. this is a test\"\n",
    "result= re.split(r'(\\s)',text) #rsplits from all the white spaces\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22bc781b-8840-44df-9cc5-f80632a3f8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', ' ', 'world', '.', '', ' ', 'this', ' ', 'is', ' ', 'a', ' ', 'test']\n"
     ]
    }
   ],
   "source": [
    "result=re.split(r'([,.]|\\s)',text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5ae56ab-50a8-4cdd-92ee-9ab45cca4a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world', '.', 'this', 'is', 'a', 'test']\n"
     ]
    }
   ],
   "source": [
    "result=[item for item in result if item.strip()] #remove white space in the list\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683702b6-cb71-4de8-941e-6e7fb0ec3284",
   "metadata": {},
   "source": [
    "in tokenization to keep or to not keep spaces it all depends on the problem statemenet that we are dealing with that means some problems that need exactly same structure as the input require white spaces, for example a python code is space sensitive therefore in that case we need not to remove white spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e7789a-2279-4ccf-a1cc-a9279766e9d9",
   "metadata": {},
   "source": [
    "**Appplying this tokenizing knowledge on our dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b6450f09-645a-4dde-9a58-309eeef01a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "token= re.split(r'([,.;!_?@$%^&*()+=\\'\\#]|--|\\s)', raw_text)\n",
    "token=[item for item in token if item.strip()]\n",
    "print(token[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d7a64352-8b81-436d-acb4-4efbec825aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4585\n"
     ]
    }
   ],
   "source": [
    "print(len(token))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331ec465-70fe-49de-b35b-ae9e9fefa406",
   "metadata": {},
   "source": [
    "# **Step 2: Creating token IDs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd91668-5fd2-4bf2-852e-74eca4d59fe1",
   "metadata": {},
   "source": [
    "tokenized text -> vocabulary (map all in alphabetical order then give unique ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "aae5df78-4d82-499f-9b81-3a2a41b09eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1173\n"
     ]
    }
   ],
   "source": [
    "words= sorted(set(token))\n",
    "vocab_size= len(words)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb2e441-88d2-4c75-8a53-dea664bebe40",
   "metadata": {},
   "source": [
    "thus now we have 1191 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d8e31687-1328-4b33-879e-9147792f39aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab={token:integer for integer, token in enumerate(words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "28b25fdb-0507-4556-b819-2ed9ec319ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "('\"Ah', 2)\n",
      "('\"Be', 3)\n",
      "('\"Begin', 4)\n",
      "('\"By', 5)\n",
      "('\"Come', 6)\n",
      "('\"Destroyed', 7)\n",
      "('\"Don', 8)\n",
      "('\"Gisburns\"', 9)\n",
      "('\"Grindles', 10)\n",
      "('\"Hang', 11)\n",
      "('\"Has', 12)\n",
      "('\"How', 13)\n",
      "('\"I', 14)\n",
      "('\"If', 15)\n",
      "('\"It', 16)\n",
      "('\"Jack', 17)\n",
      "('\"Money', 18)\n",
      "('\"Moon-dancers\"', 19)\n",
      "('\"Mr', 20)\n",
      "('\"Mrs', 21)\n",
      "('\"My', 22)\n",
      "('\"Never', 23)\n",
      "('\"Of', 24)\n",
      "('\"Oh', 25)\n",
      "('\"Once', 26)\n",
      "('\"Only', 27)\n",
      "('\"Or', 28)\n",
      "('\"That', 29)\n",
      "('\"The', 30)\n",
      "('\"Then', 31)\n",
      "('\"There', 32)\n",
      "('\"There:', 33)\n",
      "('\"This', 34)\n",
      "('\"We', 35)\n",
      "('\"Well', 36)\n",
      "('\"What', 37)\n",
      "('\"When', 38)\n",
      "('\"Why', 39)\n",
      "('\"Yes', 40)\n",
      "('\"You', 41)\n",
      "('\"but', 42)\n",
      "('\"deadening', 43)\n",
      "('\"dragged', 44)\n",
      "('\"effects\"', 45)\n",
      "('\"interesting\":', 46)\n",
      "('\"lift', 47)\n",
      "('\"obituary\"', 48)\n",
      "('\"strongest', 49)\n",
      "('\"strongly\"', 50)\n"
     ]
    }
   ],
   "source": [
    "for i,item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i>=50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77f2d67-b68c-4ff8-b04c-8a6abc42485c",
   "metadata": {},
   "source": [
    "**creating tokenizer class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cd8b222b-7827-4021-8b6e-74c414ad98ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class tokenize:\n",
    "    def __init__(self,vocab):\n",
    "        self.str_to_int=vocab\n",
    "        self.int_to_str={i:s for s,i in vocab.items()}\n",
    "    def encode(self,text):\n",
    "        token= re.split(r'([,.;!_@$%^&*()+=\\'\\#]|--|\\s)',text)\n",
    "        token=[item.strip() for item in token if item.strip()]\n",
    "        ids=[self.str_to_int[s] for s in token]\n",
    "        return ids\n",
    "    def decode(self,ids):\n",
    "        #recreating the text from tokens\n",
    "        text=\" \".join([self.int_to_str[i] for i in ids])\n",
    "        text=re.sub(r'\\s+([,.?!\"()\\'])',r'\\1',text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dc9df9f3-c80d-41a6-8ac2-57e84647173d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[93, 87, 182, 1043, 97, 82, 858, 147, 289, 518, 56, 1042, 147, 533, 57]\n",
      "I HAD always thought Jack Gisburn rather a cheap genius -- though a good.\n"
     ]
    }
   ],
   "source": [
    "tokens=tokenize(vocab)\n",
    "text= \"I HAD always thought Jack Gisburn rather a cheap genius--though a good.\"\n",
    "ids=tokens.encode(text)\n",
    "txt=tokens.decode(ids)\n",
    "print(ids)\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8074c3e9-13c1-44f2-b5b7-977cb25d605f",
   "metadata": {},
   "source": [
    "**what if encounter a awork out of our vocaabulary?**\n",
    "we use <|unk|> & <|endoftext|> tokens in the vocabulary to tckle this situation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fd0d6403-3a61-4085-9c74-9cabe9500d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "alltoken=sorted(list(token))\n",
    "alltoken.extend([\"<|endoftext|>\",\"<|unk|>\"])\n",
    "vocab2={token:integer for integer,token in enumerate(alltoken)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "315d8415-d647-45cb-89fa-0259e0ea9a3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1175"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab2.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e3de3a73-643b-4854-920b-5afc1d5debc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class tokenize:\n",
    "    def __init__(self,cab):\n",
    "        self.str_to_int=cab\n",
    "        self.int_to_str={i:s for s,i in cab.items()}\n",
    "    def encode(self,text):\n",
    "        token= re.split(r'([,.;!_@$%^&*()+=\\'\\#]|--|\\s)',text)\n",
    "        token=[item.strip() for item in token if item.strip()]\n",
    "        token=[item if item in self.str_to_int\n",
    "            else \"<|unk|>\" for item in token]\n",
    "        ids=[self.str_to_int[s] for s in token]\n",
    "        return ids\n",
    "    def decode(self,ids):\n",
    "        #recreating the text from tokens\n",
    "        text=\" \".join([self.int_to_str[i] for i in ids])\n",
    "        text=re.sub(r'\\s+([,.?!\"()\\'])',r'\\1',text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d764de37-f43d-4766-ab19-11bfb2536668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea?<|endoftext|>In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "tokenize=tokenize(vocab2)\n",
    "t1=\"Hello, do you like tea?\"\n",
    "t2=\"In the sunlit terraces of the palace.\"\n",
    "text=\"<|endoftext|>\".join((t1,t2))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "01d8f255-bed9-4615-9dac-c3287105d4ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, do you like <|unk|> the sunlit terraces of the <|unk|>.'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize.decode(tokenize.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dd73ed-e13b-439c-b567-2ce9e212f975",
   "metadata": {},
   "source": [
    "# \"BYTE_PAIR_Encoding\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4855fbac-98e1-4697-8a4b-1c855d2d041d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.12.0-cp310-cp310-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\hp\\miniconda3\\envs\\tf210\\lib\\site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\hp\\miniconda3\\envs\\tf210\\lib\\site-packages (from tiktoken) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\hp\\miniconda3\\envs\\tf210\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\miniconda3\\envs\\tf210\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\miniconda3\\envs\\tf210\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\miniconda3\\envs\\tf210\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2025.6.15)\n",
      "Downloading tiktoken-0.12.0-cp310-cp310-win_amd64.whl (879 kB)\n",
      "   ---------------------------------------- 0.0/879.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/879.4 kB ? eta -:--:--\n",
      "   ----------------------- ---------------- 524.3/879.4 kB 1.5 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 524.3/879.4 kB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 879.4/879.4 kB 1.2 MB/s  0:00:00\n",
      "Installing collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.12.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "eff818ec-9b85-4800-a70c-67a686f8c38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefc2487-d550-4134-ae90-2af3cc2ffe90",
   "metadata": {},
   "source": [
    "Creating an instance of tokenizer class from tiktoken, it works just like our version of tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6df4ab74-3b14-4a4c-a069-a9bd4ae82dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5b1a2f17-2f3a-40ee-8d21-dfda127f5cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 50256, 818, 262, 4252, 18250, 8812, 2114, 286, 262, 20562, 13]\n"
     ]
    }
   ],
   "source": [
    "text=(\"Hello, do you like tea?<|endoftext|>In the sunlit terraces of the palace.\")\n",
    "integer=tokenizer.encode(text,allowed_special={'<|endoftext|>'})\n",
    "print(integer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4287646d-e100-4aab-b4e4-23ce65f4576b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea?<|endoftext|>In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "string=tokenizer.decode(integer)\n",
    "print(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f2920a-ef44-41c1-9e44-d07e69ebd5bb",
   "metadata": {},
   "source": [
    "_we can clearly see here, it handles both <|unk|> and <|endoftext|> tokens easily, as it is using BPE at root level thus shows no error and runs smoothly, moreover it is also trained on larger dataset_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f843bb-b3cb-4472-967a-84023d3c0ec8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
